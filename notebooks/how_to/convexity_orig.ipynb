{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory added to sys.path: /Users/kenjileong/Documents/GitHub/awesome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/smu_opt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\",\"..\"))\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Verify if the parent directory is added\n",
    "print(\"Parent directory added to sys.path:\", parent_dir)\n",
    "\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "from awesome.util.path_tools import get_project_root_path\n",
    "from awesome.run.functions import plot_as_image, get_mpl_figure, plot_mask\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from awesome.model.zoo import Zoo\n",
    "from awesome.model.convex_net import ConvexNextNet\n",
    "from awesome.run.runner import seed_all\n",
    "from awesome.util.torch import tensorify\n",
    "import torch\n",
    "from matplotlib.colors import get_named_colors_mapping\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ioff()\n",
    "zoo = Zoo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-To: Convexity\n",
    "\n",
    "This is a simple demonstration, how to use our convexity prior to get a convex segmentation of some unaries.\n",
    "We used the \"sequential\" fit as terminology in our manuscript, one can also discribe it as post-processing to some initial segmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider the following example:\n",
    "\n",
    "One tries to get a segmentation of a tomato, to calculate e.g. its volume, so occlusions should be included in the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/kenjileong/Documents/GitHub/awesome/notebooks/how_to/notebooks/how_to/data/cocktail-tomatoes.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     nx,ny,nc \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m---> 14\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdown_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m fig \u001b[38;5;241m=\u001b[39m plot_as_image(img, variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTomato\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m fig\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(path, down_scale)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_image\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, down_scale: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m----> 4\u001b[0m     img_pil\u001b[38;5;241m=\u001b[39m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img_pil\u001b[38;5;241m.\u001b[39msize \n\u001b[1;32m      6\u001b[0m     newsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(width\u001b[38;5;241m/\u001b[39mdown_scale), \u001b[38;5;28mint\u001b[39m(height\u001b[38;5;241m/\u001b[39mdown_scale))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/smu_opt/lib/python3.10/site-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/kenjileong/Documents/GitHub/awesome/notebooks/how_to/notebooks/how_to/data/cocktail-tomatoes.jpg'"
     ]
    }
   ],
   "source": [
    "img_dir=\"notebooks/how_to/data/cocktail-tomatoes.jpg\"\n",
    "\n",
    "def load_image(path: str, down_scale: int = 1) -> np.ndarray:\n",
    "    img_pil=Image.open(path)\n",
    "    width, height = img_pil.size \n",
    "    newsize = (int(width/down_scale), int(height/down_scale))\n",
    "    img_pil = img_pil.resize(newsize)\n",
    "\n",
    "    img= np.array(img_pil, dtype='float')/255.0\n",
    "    img = img[:,:,0:3]\n",
    "    nx,ny,nc = img.shape\n",
    "    return img\n",
    "\n",
    "img = load_image(img_dir, down_scale=2)\n",
    "\n",
    "fig = plot_as_image(img, variable_name=\"Tomato\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets try to get a ruff segmentation, like with a simple thresholding, of course, one can also use any other segmentation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = img[:,:,0]-img[:,:,1]-img[:,:,2] - 0.5\n",
    "\n",
    "likelihood = torch.sigmoid(torch.tensor(1-likelihood).float())\n",
    "likelihood = likelihood - torch.min(likelihood)\n",
    "likelihood = likelihood / torch.max(likelihood)\n",
    "likelihood = (likelihood<0.5).float()\n",
    "\n",
    "fig = plot_mask(img, likelihood, filled_contours=False, lined_contours=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation covers the tomato, quite well, but the leafs block some parts.\n",
    "\n",
    "So, lets define a convexity prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_cuda = True\n",
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "height, width = likelihood.shape[-2:]\n",
    "\n",
    "def get_model_device(model):\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "# We need a coordinate grid, which we use to query our implicit representation.\n",
    "def create_grid(image_shape, device: torch.device):\n",
    "    ny, nx = image_shape\n",
    "    x = torch.arange(0, nx, device=device)\n",
    "    y = torch.arange(0, ny, device=device)\n",
    "    xx, yy = torch.meshgrid(x, y, indexing='xy')\n",
    "    grid = torch.stack((xx, yy), dim=0)\n",
    "    batched_input = grid.unsqueeze(0).float() / torch.tensor([nx, ny], device=device).float().unsqueeze(-1).unsqueeze(-1)\n",
    "    return batched_input\n",
    "x = create_grid((height, width), device)\n",
    "\n",
    "# We define the FG as 0, so we need to invert the mask\n",
    "unaries = 1 - likelihood.to(device)\n",
    "\n",
    "seed_all(0)\n",
    "\n",
    "model = ConvexNextNet(n_hidden_layers=1)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def plot_loss(loss: torch.Tensor):\n",
    "    fig, ax = get_mpl_figure(1, 1)\n",
    "    ax.plot(loss.detach().cpu().numpy())\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Training Loss\")\n",
    "    return fig\n",
    "\n",
    "# Using Adamax as optimizer for the flow net was giving us slightly better results than Adam, but feel free to experiment.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)  \n",
    "num_epochs = 2000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model to the data. This will take a while, so we will only train for 2000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from awesome.measures.se import SE\n",
    "\n",
    "\n",
    "def train(\n",
    "        optimizer: torch.optim.Optimizer, \n",
    "        model: ConvexNextNet, \n",
    "        unaries: torch.Tensor, \n",
    "        num_epochs: int, \n",
    "        fg_weight: float = 0.66,\n",
    "        ):\n",
    "    # Train the model\n",
    "    loss_full = torch.zeros(num_epochs, dtype=torch.float32)\n",
    "    model.train()\n",
    "\n",
    "    criterion = SE(reduction='none')\n",
    "\n",
    "    batched_input = create_grid(unaries.shape[-2:], unaries.device)\n",
    "    it = tqdm(range(num_epochs), total=num_epochs, desc=f\"Training\")\n",
    "    batched_unaries = unaries.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    background_mask = batched_unaries == 1.\n",
    "    foreground_mask = ~background_mask\n",
    "\n",
    "    bg_length = background_mask.sum()\n",
    "    fg_length = foreground_mask.sum()\n",
    "\n",
    "    fg_weight = tensorify(fg_weight, device=unaries.device)\n",
    "\n",
    "    for epoch in it:\n",
    "        \n",
    "        out = torch.sigmoid(model(batched_input))\n",
    "        if torch.isnan(out).any():\n",
    "            raise ValueError(\"Output is nan\")\n",
    "\n",
    "        fg_loss = criterion(out[foreground_mask], batched_unaries[foreground_mask])\n",
    "        bg_loss = criterion(out[background_mask], batched_unaries[background_mask])\n",
    "\n",
    "        # We are weighting the fg and bg differently to get proper tomato segmentation\n",
    "        loss = (1 - fg_weight) * (bg_loss.sum() / bg_length) + (fg_weight * (fg_loss.sum() / fg_length))\n",
    "\n",
    "        loss_full[epoch] = loss.detach().cpu()\n",
    "            \n",
    "        # Backprpagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # After Each epoch we enforce the convexity of the convex net to ensure the convexity of the model => Weights should be positive\n",
    "        model.enforce_convexity()\n",
    "\n",
    "        if (epoch+1) % 50 == 0 or epoch == 0:\n",
    "            it.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "    return model, loss_full\n",
    "\n",
    "tm, loss = train(optimizer, model, unaries, num_epochs, fg_weight=0.4)\n",
    "fig = plot_loss(loss)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the trained model, we can now inference and get the segmentation of the tomato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # We query the model with a grid of coordinates. The model will return the likelihood of each pixel being foreground.\n",
    "    # We can also create a super resolution grid to get a higher resolution output.\n",
    "    in_ = create_grid(unaries.shape[-2:], unaries.device).to(get_model_device(model))\n",
    "    path_connected_likelihood = model(in_).squeeze().cpu()\n",
    "\n",
    "# We can now threshold the output to get a binary mask.\n",
    "path_connected_likelihood_mask = torch.sigmoid(path_connected_likelihood) < 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voila! We have a *provable* convex segmentation of the tomato!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = get_named_colors_mapping().get(\"tab:orange\")\n",
    "plot_mask(img, path_connected_likelihood_mask, color=color, variable_name=\"Output of the model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smu_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
